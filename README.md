# ğŸ“Œ Proyecto ETL Universal con Python

Este proyecto lo realicÃ© con el objetivo de crear un **pipeline ETL (Extract, Transform, Load)** totalmente modular y universal, que pueda trabajar con cualquier archivo CSV o Excel sin importar su estructura.

La idea principal fue organizar el cÃ³digo en diferentes mÃ³dulos que representen cada fase del proceso: **ExtracciÃ³n, Limpieza, TransformaciÃ³n y Carga**, ademÃ¡s de un archivo de configuraciÃ³n que permite cambiar parÃ¡metros sin necesidad de modificar el cÃ³digo principal.

---

## ğŸš€ Funcionalidades principales

âœ… **Extract**

* Permite leer cualquier archivo `.csv` o `.xlsx`.
* Se puede configurar la ruta del archivo desde `Config/ConfigBig.py`.

âœ… **Clean**

* Elimina duplicados en el dataset.
* Maneja valores nulos (`mean`, `median`, `mode` o ignorar).
* Limpia espacios innecesarios en columnas de texto.
* Todo esto se aplica de forma **universal a cualquier dataset**.

âœ… **Transform**

* Normaliza todas las columnas numÃ©ricas en valores entre `0 y 1`.
* Permite renombrar columnas de manera sencilla.

âœ… **Load**

* Guarda los datos procesados en un nuevo archivo CSV dentro de la carpeta `output/`.

âœ… **main.py**

* Orquesta todo el proceso ETL.
* Se ejecuta en 4 pasos: **Extract â†’ Clean â†’ Transform â†’ Load**.

---

## ğŸ“‚ Estructura del proyecto

```
BASEDEDATOSBIGDATA/
â”‚â”€â”€ Config/
â”‚   â””â”€â”€ ConfigBig.py
â”‚
â”‚â”€â”€ Extract/
â”‚   â”œâ”€â”€ BigDataExtract.py
â”‚   â””â”€â”€ Clean/
â”‚       â””â”€â”€ Clean.py
â”‚
â”‚â”€â”€ Transform/
â”‚   â””â”€â”€ BigDataTransform.py
â”‚
â”‚â”€â”€ Load/
â”‚   â””â”€â”€ BigDataLoad.py
â”‚
â”‚â”€â”€ main.py
â”‚â”€â”€ Pokemon.csv        # Dataset de prueba
â”‚â”€â”€ requirements.txt
â”‚â”€â”€ README.md
```

---

## âš™ï¸ InstalaciÃ³n y requisitos

1. Clonar este repositorio o copiar la estructura.
2. Crear un entorno virtual (opcional pero recomendado):

```bash
python -m venv venv
source venv/bin/activate   # Linux/Mac
venv\Scripts\activate      # Windows
```

3. Instalar las dependencias:

```bash
pip install -r requirements.txt
```

ğŸ“Œ `requirements.txt` contiene:

```txt
pandas
numpy
```

---

## â–¶ï¸ Uso

1. Coloca tu archivo `.csv` o `.xlsx` en la carpeta principal.
2. Ajusta la ruta en `Config/ConfigBig.py` si quieres usar otro archivo distinto a `Pokemon.csv`.
3. Ejecuta el flujo ETL:

```bash
python main.py
```

4. El resultado se guardarÃ¡ automÃ¡ticamente en la carpeta `output/` como `cleaned_output.csv`.

---

## ğŸ“Š Ejemplo de salida

Si usamos el dataset de prueba `Pokemon.csv`, el flujo harÃ¡ lo siguiente:

* CargarÃ¡ todos los registros.
* LimpiarÃ¡ duplicados, valores faltantes y espacios.
* NormalizarÃ¡ las estadÃ­sticas (`HP`, `Attack`, `Defense`, etc.) entre 0 y 1.
* GuardarÃ¡ un archivo final en `output/cleaned_output.csv`.

---

## ğŸ“‘ About Dataset

Este dataset incluye **721 PokÃ©mon**, con su nÃºmero, nombre, primer y segundo tipo, y estadÃ­sticas bÃ¡sicas: **HP, Attack, Defense, Special Attack, Special Defense y Speed**.
Ha sido usado como recurso educativo para enseÃ±ar estadÃ­stica, y tambiÃ©n puede servir como introducciÃ³n geek a temas de machine learning.

ğŸ“Œ El dataset describe atributos del videojuego PokÃ©mon (no cartas ni PokÃ©mon Go).

Los atributos son:

* **#**: ID para cada PokÃ©mon
* **Name**: Nombre de cada PokÃ©mon
* **Type 1**: Tipo principal (determina debilidades y resistencias)
* **Type 2**: Tipo secundario (si aplica)
* **Total**: Suma de todas las estadÃ­sticas, indicador general de fuerza
* **HP**: Puntos de vida, resistencia antes de debilitarse
* **Attack**: Potencia base de ataques fÃ­sicos
* **Defense**: Resistencia frente a ataques fÃ­sicos
* **SP Atk**: Potencia base de ataques especiales (ej. Fire Blast, Bubble Beam)
* **SP Def**: Resistencia frente a ataques especiales
* **Speed**: Determina quÃ© PokÃ©mon ataca primero en cada turno

---

## ğŸ”® ConclusiÃ³n

Este proyecto me permitiÃ³ practicar la **arquitectura de un ETL real**, modularizar el cÃ³digo en Python y crear un sistema **universal** que pueda trabajar con cualquier dataset.
En el futuro se le pueden agregar mÃ¡s transformaciones y hasta conexiÃ³n con bases de datos para automatizar todo el proceso.

